{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-repo/pure_discovery_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🫀 Pure Discovery: Self-Supervised Cardiac Pattern Learning\n",
    "\n",
    "**Objective**: Discover natural cardiac patterns in 30k unlabeled ECG+PPG dataset using self-supervised learning, without relying on pre-defined medical classifications.\n",
    "\n",
    "## 🎯 What This Notebook Does:\n",
    "1. **Self-Supervised Pretraining**: Learn representations from unlabeled ECG+PPG pairs\n",
    "2. **Pattern Discovery**: Use clustering to find natural cardiac groupings\n",
    "3. **Clinical Interpretation**: Analyze discovered patterns for medical relevance\n",
    "4. **Novel Insights**: Potentially discover new cardiac risk patterns\n",
    "\n",
    "## 📊 Expected Outcomes:\n",
    "- 5-15 natural cardiac pattern clusters\n",
    "- Clinical interpretation of each pattern\n",
    "- Better real-world performance than supervised approaches\n",
    "- Potential novel medical discoveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Colab\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install numpy pandas matplotlib seaborn scikit-learn\n",
    "!pip install scipy wfdb imbalanced-learn\n",
    "!pip install umap-learn  # For dimensionality reduction\n",
    "!pip install plotly  # For interactive plots\n",
    "!pip install tqdm  # Progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual repo)\n",
    "!git clone https://github.com/your-username/ecg-ppg-analysis.git\n",
    "%cd ecg-ppg-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our custom modules\n",
    "from src.config import DEVICE, ECG_FS, PPG_FS, BATCH_SIZE\n",
    "from src.utils import seed_everything, setup_logging\n",
    "from src.self_supervised import SelfSupervisedDataset, create_self_supervised_dataloader\n",
    "from src.contrastive_trainer import SelfSupervisedEncoder, ContrastiveTrainer\n",
    "from src.pattern_analyzer import PatternAnalyzer\n",
    "from src.validation import validate_config\n",
    "\n",
    "# Setup\n",
    "seed_everything(42)\n",
    "setup_logging()\n",
    "validate_config()\n",
    "\n",
    "print(f\"🔥 Using device: {DEVICE}\")\n",
    "print(f\"📊 ECG sampling rate: {ECG_FS} Hz\")\n",
    "print(f\"💓 PPG sampling rate: {PPG_FS} Hz\")\n",
    "print(f\"📦 Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📂 Data Loading and Preparation\n",
    "\n",
    "**Important**: Replace the data loading section below with your actual 30k dataset loading code.\n",
    "This section assumes you have ECG and PPG signals stored in a format you can load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# REPLACE THIS SECTION WITH YOUR DATA LOADING CODE\n",
    "# ========================================\n",
    "\n",
    "def load_your_30k_dataset():\n",
    "    \"\"\"\n",
    "    Replace this function with your actual data loading logic.\n",
    "    \n",
    "    Expected output:\n",
    "    - ecg_signals: List of numpy arrays, each containing ECG signal for one patient\n",
    "    - ppg_signals: List of numpy arrays, each containing PPG signal for one patient  \n",
    "    - patient_ids: List of patient identifiers (optional)\n",
    "    \n",
    "    Note: Each patient should have one ECG+PPG pair as mentioned in your requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example structure - REPLACE WITH YOUR ACTUAL LOADING\n",
    "    data_path = \"/path/to/your/30k/dataset\"\n",
    "    \n",
    "    ecg_signals = []\n",
    "    ppg_signals = []\n",
    "    patient_ids = []\n",
    "    \n",
    "    # Example loading patterns:\n",
    "    \n",
    "    # Option 1: If you have CSV files\n",
    "    # for patient_id in range(30000):\n",
    "    #     ecg_file = f\"{data_path}/patient_{patient_id}_ecg.csv\"\n",
    "    #     ppg_file = f\"{data_path}/patient_{patient_id}_ppg.csv\"\n",
    "    #     \n",
    "    #     if os.path.exists(ecg_file) and os.path.exists(ppg_file):\n",
    "    #         ecg = pd.read_csv(ecg_file)['signal'].values\n",
    "    #         ppg = pd.read_csv(ppg_file)['signal'].values\n",
    "    #         \n",
    "    #         ecg_signals.append(ecg)\n",
    "    #         ppg_signals.append(ppg)\n",
    "    #         patient_ids.append(f\"patient_{patient_id}\")\n",
    "    \n",
    "    # Option 2: If you have HDF5 or numpy files\n",
    "    # import h5py\n",
    "    # with h5py.File(f\"{data_path}/dataset.h5\", 'r') as f:\n",
    "    #     ecg_signals = [f['ecg'][i] for i in range(len(f['ecg']))]\n",
    "    #     ppg_signals = [f['ppg'][i] for i in range(len(f['ppg']))]\n",
    "    #     patient_ids = [f'patient_{i}' for i in range(len(f['ecg']))]\n",
    "    \n",
    "    # Option 3: If you have WFDB format\n",
    "    # import wfdb\n",
    "    # for record_name in record_list:\n",
    "    #     signals, fields = wfdb.rdsamp(f\"{data_path}/{record_name}\")\n",
    "    #     ecg = signals[:, 0]  # Assuming ECG is first channel\n",
    "    #     ppg = signals[:, 1]  # Assuming PPG is second channel\n",
    "    #     \n",
    "    #     ecg_signals.append(ecg)\n",
    "    #     ppg_signals.append(ppg)\n",
    "    #     patient_ids.append(record_name)\n",
    "    \n",
    "    # PLACEHOLDER: Generate synthetic data for demo\n",
    "    print(\"⚠️  WARNING: Using synthetic data for demo. Replace with your actual dataset!\")\n",
    "    \n",
    "    n_patients = 100  # Use smaller number for demo\n",
    "    \n",
    "    for i in range(n_patients):\n",
    "        # Generate synthetic ECG (10 minutes at 360 Hz)\n",
    "        duration = 600  # 10 minutes\n",
    "        ecg_samples = int(duration * ECG_FS)\n",
    "        t = np.linspace(0, duration, ecg_samples)\n",
    "        \n",
    "        # Simulate ECG with different patterns\n",
    "        hr = np.random.normal(75, 15)  # Heart rate variability\n",
    "        hr = np.clip(hr, 50, 120)\n",
    "        \n",
    "        ecg = np.zeros_like(t)\n",
    "        for beat_time in np.arange(0, duration, 60/hr):\n",
    "            beat_mask = (t >= beat_time) & (t < beat_time + 0.8)\n",
    "            if np.any(beat_mask):\n",
    "                beat_signal = np.exp(-((t[beat_mask] - beat_time - 0.15)**2) / 0.01)\n",
    "                ecg[beat_mask] += beat_signal\n",
    "        \n",
    "        # Add noise and artifacts\n",
    "        ecg += 0.1 * np.random.randn(len(ecg))\n",
    "        \n",
    "        # Generate corresponding PPG\n",
    "        ppg_samples = int(duration * PPG_FS)\n",
    "        t_ppg = np.linspace(0, duration, ppg_samples)\n",
    "        \n",
    "        ppg = np.zeros_like(t_ppg)\n",
    "        for beat_time in np.arange(0, duration, 60/hr):\n",
    "            beat_mask = (t_ppg >= beat_time) & (t_ppg < beat_time + 1.2)\n",
    "            if np.any(beat_mask):\n",
    "                beat_signal = np.exp(-((t_ppg[beat_mask] - beat_time - 0.3)**2) / 0.05)\n",
    "                ppg[beat_mask] += beat_signal\n",
    "        \n",
    "        ppg += 0.05 * np.random.randn(len(ppg))\n",
    "        ppg += 1.0  # Add baseline\n",
    "        \n",
    "        ecg_signals.append(ecg.astype(np.float32))\n",
    "        ppg_signals.append(ppg.astype(np.float32))\n",
    "        patient_ids.append(f\"demo_patient_{i}\")\n",
    "    \n",
    "    return ecg_signals, ppg_signals, patient_ids\n",
    "\n",
    "# Load your dataset\n",
    "print(\"📊 Loading dataset...\")\n",
    "ecg_signals, ppg_signals, patient_ids = load_your_30k_dataset()\n",
    "\n",
    "print(f\"✅ Loaded {len(ecg_signals)} ECG signals\")\n",
    "print(f\"✅ Loaded {len(ppg_signals)} PPG signals\") \n",
    "print(f\"✅ Patient IDs: {len(patient_ids)}\")\n",
    "\n",
    "# Basic statistics\n",
    "ecg_lengths = [len(sig) for sig in ecg_signals]\n",
    "ppg_lengths = [len(sig) for sig in ppg_signals]\n",
    "\n",
    "print(f\"📈 ECG signal lengths: {np.mean(ecg_lengths):.0f} ± {np.std(ecg_lengths):.0f} samples\")\n",
    "print(f\"💓 PPG signal lengths: {np.mean(ppg_lengths):.0f} ± {np.std(ppg_lengths):.0f} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess data quality before training\n",
    "from src.validation import SignalValidator\n",
    "\n",
    "validator = SignalValidator()\n",
    "quality_scores = []\n",
    "valid_pairs = []\n",
    "\n",
    "print(\"🔍 Assessing signal quality...\")\n",
    "\n",
    "for i, (ecg, ppg) in enumerate(tqdm(zip(ecg_signals[:100], ppg_signals[:100]), desc=\"Quality check\")):\n",
    "    # Basic validation\n",
    "    ecg_checks = validator.validate_signal_basic(ecg, \"ECG\")\n",
    "    ppg_checks = validator.validate_signal_basic(ppg, \"PPG\")\n",
    "    \n",
    "    if all(ecg_checks.values()) and all(ppg_checks.values()):\n",
    "        # Quality assessment\n",
    "        ecg_quality = validator.validate_signal_quality(ecg, ECG_FS)\n",
    "        ppg_quality = validator.validate_signal_quality(ppg, PPG_FS)\n",
    "        \n",
    "        avg_quality = (ecg_quality['quality_score'] + ppg_quality['quality_score']) / 2\n",
    "        quality_scores.append(avg_quality)\n",
    "        \n",
    "        if avg_quality > 0.3:  # Minimum quality threshold\n",
    "            valid_pairs.append(i)\n",
    "\n",
    "print(f\"✅ Quality assessment complete:\")\n",
    "print(f\"   Average quality score: {np.mean(quality_scores):.3f}\")\n",
    "print(f\"   Valid pairs: {len(valid_pairs)}/{len(ecg_signals[:100])}\")\n",
    "print(f\"   Quality distribution: min={np.min(quality_scores):.3f}, max={np.max(quality_scores):.3f}\")\n",
    "\n",
    "# Plot quality distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(quality_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(quality_scores), color='red', linestyle='--', label=f'Mean: {np.mean(quality_scores):.3f}')\n",
    "plt.xlabel('Quality Score')\n",
    "plt.ylabel('Number of Signals')\n",
    "plt.title('Signal Quality Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "valid_ratio = len(valid_pairs) / len(ecg_signals[:100])\n",
    "plt.pie([valid_ratio, 1-valid_ratio], labels=['Valid', 'Poor Quality'], \n",
    "        autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "plt.title('Data Quality Assessment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Sample Signal Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample signals\n",
    "n_samples = 3\n",
    "fig, axes = plt.subplots(n_samples, 2, figsize=(15, n_samples * 3))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    idx = valid_pairs[i] if i < len(valid_pairs) else i\n",
    "    \n",
    "    ecg = ecg_signals[idx]\n",
    "    ppg = ppg_signals[idx]\n",
    "    \n",
    "    # Show first 10 seconds\n",
    "    ecg_segment = ecg[:ECG_FS * 10]\n",
    "    ppg_segment = ppg[:PPG_FS * 10]\n",
    "    \n",
    "    t_ecg = np.arange(len(ecg_segment)) / ECG_FS\n",
    "    t_ppg = np.arange(len(ppg_segment)) / PPG_FS\n",
    "    \n",
    "    # Plot ECG\n",
    "    axes[i, 0].plot(t_ecg, ecg_segment, 'b-', linewidth=0.8)\n",
    "    axes[i, 0].set_title(f'Patient {patient_ids[idx]} - ECG Signal')\n",
    "    axes[i, 0].set_xlabel('Time (seconds)')\n",
    "    axes[i, 0].set_ylabel('Amplitude (mV)')\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot PPG\n",
    "    axes[i, 1].plot(t_ppg, ppg_segment, 'r-', linewidth=0.8)\n",
    "    axes[i, 1].set_title(f'Patient {patient_ids[idx]} - PPG Signal')\n",
    "    axes[i, 1].set_xlabel('Time (seconds)')\n",
    "    axes[i, 1].set_ylabel('Amplitude')\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Dataset Creation for Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only high-quality signals\n",
    "filtered_ecg = [ecg_signals[i] for i in valid_pairs]\n",
    "filtered_ppg = [ppg_signals[i] for i in valid_pairs]\n",
    "filtered_ids = [patient_ids[i] for i in valid_pairs]\n",
    "\n",
    "print(f\"📊 Using {len(filtered_ecg)} high-quality signal pairs for training\")\n",
    "\n",
    "# Split into train/validation\n",
    "split_idx = int(0.8 * len(filtered_ecg))\n",
    "\n",
    "train_ecg = filtered_ecg[:split_idx]\n",
    "train_ppg = filtered_ppg[:split_idx]\n",
    "train_ids = filtered_ids[:split_idx]\n",
    "\n",
    "val_ecg = filtered_ecg[split_idx:]\n",
    "val_ppg = filtered_ppg[split_idx:]\n",
    "val_ids = filtered_ids[split_idx:]\n",
    "\n",
    "print(f\"🚂 Training set: {len(train_ecg)} patients\")\n",
    "print(f\"🔍 Validation set: {len(val_ecg)} patients\")\n",
    "\n",
    "# Create self-supervised datasets\n",
    "print(\"🏗️ Creating self-supervised datasets...\")\n",
    "\n",
    "train_loader = create_self_supervised_dataloader(\n",
    "    ecg_signals=train_ecg,\n",
    "    ppg_signals=train_ppg,\n",
    "    patient_ids=train_ids,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=True,\n",
    "    quality_filter=True,\n",
    "    overlap=0.5\n",
    ")\n",
    "\n",
    "val_loader = create_self_supervised_dataloader(\n",
    "    ecg_signals=val_ecg,\n",
    "    ppg_signals=val_ppg, \n",
    "    patient_ids=val_ids,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,  # No augmentation for validation\n",
    "    quality_filter=True,\n",
    "    overlap=0.3\n",
    ")\n",
    "\n",
    "print(f\"✅ Training batches: {len(train_loader)}\")\n",
    "print(f\"✅ Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Check a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"📦 Batch shapes:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"   {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Model Architecture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize self-supervised encoder\n",
    "print(\"🧠 Initializing self-supervised encoder...\")\n",
    "\n",
    "model = SelfSupervisedEncoder(\n",
    "    ecg_seq_len=ECG_FS * 10,  # 10 seconds of ECG\n",
    "    ppg_seq_len=ECG_FS * 10,  # Resampled PPG to match ECG\n",
    "    embedding_dim=256,\n",
    "    ecg_cnn_filters=[32, 64, 64, 128],\n",
    "    ppg_cnn_filters=[16, 32, 32, 64],\n",
    "    ecg_lstm_hidden=128,\n",
    "    ppg_lstm_hidden=64,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"🔢 Total parameters: {total_params:,}\")\n",
    "print(f\"🔢 Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_ecg = sample_batch['ecg_view1'][:2].to(DEVICE)\n",
    "    test_ppg = sample_batch['ppg_view1'][:2].to(DEVICE)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    test_output = model(test_ecg, test_ppg)\n",
    "    \n",
    "    print(f\"🧪 Test output shapes:\")\n",
    "    for key, value in test_output.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"   {key}: {value.shape}\")\n",
    "\n",
    "print(\"✅ Model architecture verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Self-Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"🚀 Initializing contrastive trainer...\")\n",
    "\n",
    "trainer = ContrastiveTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=DEVICE,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    epochs=50,  # Adjust based on your computational budget\n",
    "    contrastive_weight=1.0,\n",
    "    cross_modal_weight=0.5,\n",
    "    temporal_weight=0.3,\n",
    "    temperature=0.07\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized!\")\n",
    "print(\"🔥 Starting self-supervised training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "training_history = trainer.train()\n",
    "\n",
    "print(\"🎉 Training completed!\")\n",
    "\n",
    "# Plot training curves\n",
    "trainer.plot_training_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Pattern Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned representations\n",
    "print(\"🔍 Extracting learned representations...\")\n",
    "\n",
    "# Create a combined dataloader for pattern discovery\n",
    "all_ecg = filtered_ecg\n",
    "all_ppg = filtered_ppg\n",
    "all_ids = filtered_ids\n",
    "\n",
    "discovery_loader = create_self_supervised_dataloader(\n",
    "    ecg_signals=all_ecg,\n",
    "    ppg_signals=all_ppg,\n",
    "    patient_ids=all_ids,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,  # No augmentation for analysis\n",
    "    quality_filter=False,  # Already filtered\n",
    "    overlap=0.0  # No overlap for cleaner analysis\n",
    ")\n",
    "\n",
    "# Discover patterns\n",
    "pattern_results = trainer.discover_patterns(discovery_loader)\n",
    "\n",
    "print(f\"🎯 Pattern discovery results:\")\n",
    "print(f\"   Optimal clusters: {pattern_results['optimal_clusters']}\")\n",
    "print(f\"   Silhouette score: {pattern_results['silhouette_score']:.3f}\")\n",
    "print(f\"   Total embeddings: {len(pattern_results['embeddings'])}\")\n",
    "\n",
    "# Print cluster sizes\n",
    "if pattern_results['cluster_labels'] is not None:\n",
    "    unique_labels, counts = np.unique(pattern_results['cluster_labels'], return_counts=True)\n",
    "    print(f\"\\n📊 Discovered Pattern Sizes:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        if label == -1:\n",
    "            print(f\"   Noise: {count} samples\")\n",
    "        else:\n",
    "            print(f\"   Pattern {label}: {count} samples ({count/len(pattern_results['cluster_labels'])*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"❌ Pattern discovery failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Comprehensive Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pattern_results['cluster_labels'] is not None:\n",
    "    print(\"📈 Running comprehensive pattern analysis...\")\n",
    "    \n",
    "    # Initialize pattern analyzer\n",
    "    analyzer = PatternAnalyzer()\n",
    "    \n",
    "    # Prepare data for analysis\n",
    "    discovery_dataset = discovery_loader.dataset\n",
    "    \n",
    "    # Get original signal windows for analysis\n",
    "    ecg_windows = np.array([discovery_dataset.ecg_windows[i] for i in range(len(discovery_dataset))])\n",
    "    ppg_windows = np.array([discovery_dataset.ppg_windows[i] for i in range(len(discovery_dataset))])\n",
    "    patient_indices = np.array([discovery_dataset.patient_indices[i] for i in range(len(discovery_dataset))])\n",
    "    quality_scores = np.array([discovery_dataset.window_quality_scores[i] for i in range(len(discovery_dataset))])\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    analysis_results = analyzer.analyze_discovered_patterns(\n",
    "        embeddings=pattern_results['embeddings'],\n",
    "        cluster_labels=pattern_results['cluster_labels'],\n",
    "        ecg_windows=ecg_windows,\n",
    "        ppg_windows=ppg_windows,\n",
    "        patient_indices=patient_indices,\n",
    "        quality_scores=quality_scores\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Comprehensive analysis completed!\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"🎨 Creating pattern visualizations...\")\n",
    "    viz_paths = analyzer.create_pattern_visualizations(analysis_results)\n",
    "    print(f\"📊 Generated {len(viz_paths)} visualization files\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Skipping analysis due to failed pattern discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏥 Clinical Interpretation of Discovered Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pattern_results['cluster_labels'] is not None and 'clinical_interpretation' in analysis_results:\n",
    "    print(\"🏥 Clinical Interpretation of Discovered Patterns\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    clinical_data = analysis_results['clinical_interpretation']\n",
    "    cluster_stats = analysis_results['cluster_statistics']\n",
    "    \n",
    "    for cluster_name in clinical_data.keys():\n",
    "        cluster_id = cluster_name.split('_')[1]\n",
    "        interpretation = clinical_data[cluster_name]\n",
    "        stats = cluster_stats[cluster_name]\n",
    "        \n",
    "        print(f\"\\n🫀 PATTERN {cluster_id.upper()}\")\n",
    "        print(f\"   Size: {stats['size']} windows from {stats['unique_patients']} patients\")\n",
    "        print(f\"   Average Quality: {stats.get('avg_quality', 'N/A'):.3f}\" if 'avg_quality' in stats else \"\")\n",
    "        \n",
    "        characteristics = interpretation['clinical_characteristics']\n",
    "        print(f\"\\n   📊 Clinical Characteristics:\")\n",
    "        print(f\"      Heart Rate: {interpretation['avg_heart_rate']:.1f} BPM ({characteristics['heart_rate_category']})\")\n",
    "        print(f\"      Rhythm: {characteristics['rhythm_category']}\")\n",
    "        print(f\"      Overall Pattern: {characteristics['overall_pattern']}\")\n",
    "        print(f\"      Signal Quality: {interpretation['signal_quality']:.3f}\")\n",
    "        \n",
    "        # Clinical significance\n",
    "        print(f\"\\n   🔬 Clinical Significance:\")\n",
    "        if characteristics['overall_pattern'] == 'Normal Sinus Rhythm':\n",
    "            print(f\"      ✅ This pattern represents normal cardiac activity\")\n",
    "        elif 'Arrhythmia' in characteristics['overall_pattern']:\n",
    "            print(f\"      ⚠️  This pattern may indicate irregular heart rhythm\")\n",
    "        elif 'Bradycardia' in characteristics['overall_pattern']:\n",
    "            print(f\"      🐌 This pattern shows slower than normal heart rate\")\n",
    "        elif 'Tachycardia' in characteristics['overall_pattern']:\n",
    "            print(f\"      🏃 This pattern shows faster than normal heart rate\")\n",
    "        else:\n",
    "            print(f\"      🔍 This pattern requires further clinical investigation\")\n",
    "        \n",
    "        print(f\"   📈 Prevalence: {stats['size']/len(pattern_results['cluster_labels'])*100:.1f}% of all cardiac windows\")\n",
    "        print(f\"   👥 Patient Distribution: {stats['avg_windows_per_patient']:.1f} windows per patient on average\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "    \n",
    "    # Summary insights\n",
    "    print(f\"\\n🎯 KEY INSIGHTS:\")\n",
    "    print(f\"   • Discovered {pattern_results['optimal_clusters']} distinct cardiac patterns\")\n",
    "    print(f\"   • Pattern separation quality: {pattern_results['silhouette_score']:.3f}\")\n",
    "    \n",
    "    normal_patterns = sum(1 for data in clinical_data.values() \n",
    "                         if data['clinical_characteristics']['overall_pattern'] == 'Normal Sinus Rhythm')\n",
    "    arrhythmia_patterns = sum(1 for data in clinical_data.values() \n",
    "                            if 'Arrhythmia' in data['clinical_characteristics']['overall_pattern'])\n",
    "    \n",
    "    print(f\"   • Normal patterns: {normal_patterns}\")\n",
    "    print(f\"   • Potential arrhythmia patterns: {arrhythmia_patterns}\")\n",
    "    print(f\"   • Novel/unclassified patterns: {len(clinical_data) - normal_patterns - arrhythmia_patterns}\")\n",
    "    \n",
    "    if len(clinical_data) - normal_patterns - arrhythmia_patterns > 0:\n",
    "        print(f\"\\n🚀 DISCOVERY: Found {len(clinical_data) - normal_patterns - arrhythmia_patterns} potentially novel cardiac patterns!\")\n",
    "        print(f\"   These patterns don't fit traditional medical categories and may represent:\")\n",
    "        print(f\"   • Population-specific cardiac signatures\")\n",
    "        print(f\"   • Early indicators of cardiac conditions\")\n",
    "        print(f\"   • Previously unrecognized rhythm variants\")\n",
    "        print(f\"   • Data quality or measurement artifacts\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Clinical interpretation not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Interactive Pattern Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pattern_results['cluster_labels'] is not None:\n",
    "    # Create interactive visualizations\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.express as px\n",
    "    \n",
    "    # 1. Embedding space visualization\n",
    "    if 'dimensionality_reduction' in analysis_results:\n",
    "        dim_red_data = analysis_results['dimensionality_reduction']\n",
    "        \n",
    "        if 'tsne' in dim_red_data:\n",
    "            coords = dim_red_data['tsne']['coordinates']\n",
    "            \n",
    "            # Create interactive scatter plot\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            unique_clusters = np.unique(pattern_results['cluster_labels'])\n",
    "            colors = px.colors.qualitative.Set1\n",
    "            \n",
    "            for i, cluster_id in enumerate(unique_clusters):\n",
    "                cluster_mask = pattern_results['cluster_labels'] == cluster_id\n",
    "                \n",
    "                label_name = 'Noise' if cluster_id == -1 else f'Pattern {cluster_id}'\n",
    "                color = 'gray' if cluster_id == -1 else colors[i % len(colors)]\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=coords[cluster_mask, 0],\n",
    "                    y=coords[cluster_mask, 1],\n",
    "                    mode='markers',\n",
    "                    name=label_name,\n",
    "                    marker=dict(color=color, size=5, opacity=0.7),\n",
    "                    text=[f'Patient: {pattern_results[\"patient_indices\"][j]}' for j in np.where(cluster_mask)[0]],\n",
    "                    hovertemplate='%{text}<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title='🫀 Discovered Cardiac Patterns (t-SNE Visualization)',\n",
    "                xaxis_title='t-SNE Component 1',\n",
    "                yaxis_title='t-SNE Component 2',\n",
    "                width=800,\n",
    "                height=600\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "    \n",
    "    # 2. Pattern characteristics radar chart\n",
    "    if 'clinical_interpretation' in analysis_results:\n",
    "        clinical_data = analysis_results['clinical_interpretation']\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        categories = ['Heart Rate', 'Rhythm Regularity', 'Signal Quality']\n",
    "        \n",
    "        for cluster_name, data in clinical_data.items():\n",
    "            cluster_id = cluster_name.split('_')[1]\n",
    "            \n",
    "            # Normalize values for radar chart\n",
    "            hr_norm = min(data['avg_heart_rate'] / 100, 1.0)  # Normalize to max 100 BPM\n",
    "            rhythm_norm = min(data['rhythm_regularity'] / 100, 1.0)  # Normalize\n",
    "            quality_norm = data['signal_quality']\n",
    "            \n",
    "            values = [hr_norm, rhythm_norm, quality_norm]\n",
    "            \n",
    "            fig.add_trace(go.Scatterpolar(\n",
    "                r=values + [values[0]],  # Close the shape\n",
    "                theta=categories + [categories[0]],\n",
    "                fill='toself',\n",
    "                name=f'Pattern {cluster_id}',\n",
    "                opacity=0.6\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            polar=dict(\n",
    "                radialaxis=dict(\n",
    "                    visible=True,\n",
    "                    range=[0, 1]\n",
    "                )\n",
    "            ),\n",
    "            title='📊 Pattern Characteristics Comparison',\n",
    "            width=600,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    # 3. Sample waveforms from each pattern\n",
    "    print(\"\\n🌊 Sample Waveforms from Each Discovered Pattern:\")\n",
    "    \n",
    "    unique_clusters = np.unique(pattern_results['cluster_labels'])\n",
    "    valid_clusters = [c for c in unique_clusters if c != -1]\n",
    "    \n",
    "    n_clusters = len(valid_clusters)\n",
    "    if n_clusters > 0:\n",
    "        fig, axes = plt.subplots(n_clusters, 2, figsize=(15, n_clusters * 3))\n",
    "        if n_clusters == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, cluster_id in enumerate(valid_clusters):\n",
    "            cluster_mask = pattern_results['cluster_labels'] == cluster_id\n",
    "            cluster_indices = np.where(cluster_mask)[0]\n",
    "            \n",
    "            if len(cluster_indices) > 0:\n",
    "                # Pick a representative sample\n",
    "                sample_idx = cluster_indices[len(cluster_indices)//2]  # Middle sample\n",
    "                \n",
    "                ecg_sample = ecg_windows[sample_idx]\n",
    "                ppg_sample = ppg_windows[sample_idx]\n",
    "                \n",
    "                # Show first 5 seconds\n",
    "                ecg_segment = ecg_sample[:ECG_FS * 5]\n",
    "                ppg_segment = ppg_sample[:ECG_FS * 5]  # Already resampled\n",
    "                \n",
    "                t = np.arange(len(ecg_segment)) / ECG_FS\n",
    "                \n",
    "                # Plot ECG\n",
    "                axes[i, 0].plot(t, ecg_segment, 'b-', linewidth=0.8)\n",
    "                axes[i, 0].set_title(f'Pattern {cluster_id} - ECG Sample')\n",
    "                axes[i, 0].set_xlabel('Time (seconds)')\n",
    "                axes[i, 0].set_ylabel('Amplitude')\n",
    "                axes[i, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Plot PPG\n",
    "                axes[i, 1].plot(t, ppg_segment, 'r-', linewidth=0.8)\n",
    "                axes[i, 1].set_title(f'Pattern {cluster_id} - PPG Sample')\n",
    "                axes[i, 1].set_xlabel('Time (seconds)')\n",
    "                axes[i, 1].set_ylabel('Amplitude')\n",
    "                axes[i, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Visualization not available due to failed pattern discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "print(\"💾 Saving results and model...\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"pure_discovery_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save pattern discovery results\n",
    "with open(results_dir / \"pattern_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pattern_results, f)\n",
    "\n",
    "# Save analysis results if available\n",
    "if 'analysis_results' in locals():\n",
    "    with open(results_dir / \"analysis_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "\n",
    "# Save training history\n",
    "with open(results_dir / \"training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_history, f)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'ecg_seq_len': ECG_FS * 10,\n",
    "        'ppg_seq_len': ECG_FS * 10,\n",
    "        'embedding_dim': 256,\n",
    "        'ecg_cnn_filters': [32, 64, 64, 128],\n",
    "        'ppg_cnn_filters': [16, 32, 32, 64],\n",
    "        'ecg_lstm_hidden': 128,\n",
    "        'ppg_lstm_hidden': 64,\n",
    "        'dropout': 0.3\n",
    "    }\n",
    "}, results_dir / \"self_supervised_model.pth\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "# 🫀 Pure Discovery Self-Supervised Learning Results\n",
    "\n",
    "## 📊 Dataset Summary\n",
    "- Total patients processed: {len(patient_ids)}\n",
    "- High-quality signal pairs: {len(filtered_ecg)}\n",
    "- Training patients: {len(train_ecg)}\n",
    "- Validation patients: {len(val_ecg)}\n",
    "\n",
    "## 🧠 Model Architecture\n",
    "- Total parameters: {total_params:,}\n",
    "- Embedding dimension: 256\n",
    "- ECG CNN filters: [32, 64, 64, 128]\n",
    "- PPG CNN filters: [16, 32, 32, 64]\n",
    "\n",
    "## 🎯 Pattern Discovery Results\n",
    "- Discovered patterns: {pattern_results['optimal_clusters'] if pattern_results['cluster_labels'] is not None else 'Failed'}\n",
    "- Silhouette score: {pattern_results['silhouette_score']:.3f if pattern_results['cluster_labels'] is not None else 'N/A'}\n",
    "- Total cardiac windows analyzed: {len(pattern_results['embeddings']) if pattern_results['cluster_labels'] is not None else 'N/A'}\n",
    "\n",
    "## 🏥 Clinical Insights\n",
    "\"\"\"\n",
    "\n",
    "if pattern_results['cluster_labels'] is not None and 'analysis_results' in locals():\n",
    "    clinical_data = analysis_results['clinical_interpretation']\n",
    "    \n",
    "    normal_patterns = sum(1 for data in clinical_data.values() \n",
    "                         if data['clinical_characteristics']['overall_pattern'] == 'Normal Sinus Rhythm')\n",
    "    arrhythmia_patterns = sum(1 for data in clinical_data.values() \n",
    "                            if 'Arrhythmia' in data['clinical_characteristics']['overall_pattern'])\n",
    "    novel_patterns = len(clinical_data) - normal_patterns - arrhythmia_patterns\n",
    "    \n",
    "    summary_report += f\"\"\"\n",
    "- Normal rhythm patterns: {normal_patterns}\n",
    "- Potential arrhythmia patterns: {arrhythmia_patterns}\n",
    "- Novel/unclassified patterns: {novel_patterns}\n",
    "\n",
    "### 🚀 Key Discoveries:\n",
    "\"\"\"\n",
    "    \n",
    "    if novel_patterns > 0:\n",
    "        summary_report += f\"\"\"\n",
    "- ✨ Found {novel_patterns} potentially novel cardiac patterns!\n",
    "- 🔬 These patterns don't fit traditional medical categories\n",
    "- 📈 May represent population-specific signatures or early indicators\n",
    "\"\"\"\n",
    "    else:\n",
    "        summary_report += \"\"\"\n",
    "- 📋 All discovered patterns map to known medical categories\n",
    "- ✅ Results validate existing cardiac classification systems\n",
    "\"\"\"\n",
    "else:\n",
    "    summary_report += \"\"\"\n",
    "- ❌ Pattern discovery or analysis failed\n",
    "- 🔧 Check data quality and model configuration\n",
    "\"\"\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "\n",
    "## 📁 Generated Files\n",
    "- `pattern_results.pkl`: Raw pattern discovery results\n",
    "- `analysis_results.pkl`: Comprehensive pattern analysis\n",
    "- `training_history.pkl`: Training curves and metrics\n",
    "- `self_supervised_model.pth`: Trained model checkpoint\n",
    "- `summary_report.md`: This summary report\n",
    "\n",
    "## 🎯 Next Steps\n",
    "1. **Clinical Validation**: Have cardiologists review discovered patterns\n",
    "2. **Longitudinal Analysis**: Track pattern evolution over time\n",
    "3. **Population Studies**: Compare patterns across demographics\n",
    "4. **Predictive Modeling**: Use patterns for risk stratification\n",
    "5. **Integration**: Deploy patterns in clinical decision support\n",
    "\n",
    "---\n",
    "*Generated by Pure Discovery Self-Supervised Learning System*\n",
    "*Timestamp: {pd.Timestamp.now()}*\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "with open(results_dir / \"summary_report.md\", \"w\") as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"✅ All results saved to: {results_dir}\")\n",
    "print(f\"📋 Summary report: {results_dir}/summary_report.md\")\n",
    "print(f\"🤖 Model checkpoint: {results_dir}/self_supervised_model.pth\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 PURE DISCOVERY TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if pattern_results['cluster_labels'] is not None:\n",
    "    print(f\"🎯 Successfully discovered {pattern_results['optimal_clusters']} cardiac patterns\")\n",
    "    print(f\"📊 Pattern quality score: {pattern_results['silhouette_score']:.3f}\")\n",
    "    print(f\"🫀 Analyzed {len(pattern_results['embeddings'])} cardiac windows\")\n",
    "    \n",
    "    if 'analysis_results' in locals():\n",
    "        stability = analysis_results['pattern_stability']\n",
    "        print(f\"🔄 Pattern stability: {stability['avg_patient_consistency']:.3f}\")\n",
    "        \n",
    "        if novel_patterns > 0:\n",
    "            print(f\"\\n🚀 BREAKTHROUGH: Discovered {novel_patterns} potentially novel patterns!\")\n",
    "            print(f\"   These may represent new cardiac insights not found in traditional medicine\")\n",
    "        \n",
    "    print(f\"\\n💡 Your self-supervised model can now:\")\n",
    "    print(f\"   • Classify cardiac signals into {pattern_results['optimal_clusters']} natural patterns\")\n",
    "    print(f\"   • Provide clinical interpretations for each pattern\")\n",
    "    print(f\"   • Generate rich embeddings for downstream tasks\")\n",
    "    print(f\"   • Detect novel cardiac patterns in real-time\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Pattern discovery failed - check data quality and model configuration\")\n",
    "\n",
    "print(f\"\\n📁 All results and model saved to: {results_dir}\")\n",
    "print(f\"🔄 Load model later with: torch.load('{results_dir}/self_supervised_model.pth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔮 Future Applications\n",
    "\n",
    "Your trained self-supervised model can now be used for:\n",
    "\n",
    "### 🎯 **Immediate Applications**\n",
    "1. **Real-time Pattern Classification**: Classify new ECG+PPG signals into discovered patterns\n",
    "2. **Anomaly Detection**: Identify signals that don't fit any discovered pattern\n",
    "3. **Patient Stratification**: Group patients by their dominant cardiac patterns\n",
    "4. **Quality Assessment**: Use embeddings to assess signal quality\n",
    "\n",
    "### 🚀 **Advanced Applications**\n",
    "1. **Longitudinal Monitoring**: Track pattern changes over time for individual patients\n",
    "2. **Risk Prediction**: Use patterns as features for outcome prediction models\n",
    "3. **Population Health**: Analyze pattern distributions across demographics\n",
    "4. **Drug Response**: Correlate patterns with treatment responses\n",
    "\n",
    "### 🧪 **Research Applications**\n",
    "1. **Novel Discovery**: Investigate clinical significance of novel patterns\n",
    "2. **Biomarker Development**: Use patterns as digital biomarkers\n",
    "3. **Precision Medicine**: Tailor treatments based on cardiac patterns\n",
    "4. **Clinical Trials**: Use patterns for patient stratification and endpoint assessment\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations! You've successfully implemented pure discovery self-supervised learning for cardiac pattern analysis!**\n",
    "\n",
    "This approach has discovered natural patterns in your 30k dataset without any reliance on traditional medical classifications, potentially revealing new insights into cardiac physiology and pathology."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Example integration code for your existing pipeline\n\nprint(\"🔗 INTEGRATION GUIDE FOR EXISTING PIPELINE\")\nprint(\"=\"*50)\n\nprint(\"\"\"\n# 1. MINIMAL INTEGRATION (Replace existing model)\nfrom src.model_bridge import create_model_for_existing_pipeline\n\n# Load your trained self-supervised model\nmodel = create_model_for_existing_pipeline(\n    ecg_seq_len=3600,  # 10 seconds at 360 Hz\n    ppg_seq_len=3600,  # Resampled PPG\n    pretrained_encoder_path=Path(\"pure_discovery_results/self_supervised_model.pth\")\n)\n\n# Use exactly like your original model\necg_signal = your_ecg_data  # Shape: (3600,)\nppg_signal = your_ppg_data  # Shape: (1250,) - will be resampled automatically\n\n# Get predictions with clinical explanations\nresult = model.predict_arrhythmia_with_explanation(ecg_signal, ppg_signal)\n\n# Extract standard outputs (compatible with existing code)\npredicted_class = result['predicted_class']  # 0-4 (N, S, V, F, U)\nconfidence = result['confidence']  # 0.0-1.0\nstroke_risk = result['stroke_risk_score']  # 0.0-1.0\n\n# NEW: Get clinical explanations for doctor validation\nclinical_report = model.generate_clinical_report(ecg_signal, ppg_signal, \"PATIENT_ID\")\n\"\"\")\n\nprint(\"\"\"\n# 2. BATCH PROCESSING (Multiple patients)\npatient_data = [\n    {\"ecg\": ecg1, \"ppg\": ppg1, \"id\": \"patient_001\"},\n    {\"ecg\": ecg2, \"ppg\": ppg2, \"id\": \"patient_002\"},\n    # ... more patients\n]\n\necg_batch = [p[\"ecg\"] for p in patient_data]\nppg_batch = [p[\"ppg\"] for p in patient_data]\npatient_ids = [p[\"id\"] for p in patient_data]\n\n# Batch prediction with explanations\nresults = model.batch_predict_with_explanations(ecg_batch, ppg_batch, patient_ids)\n\nfor result in results:\n    if result['success']:\n        print(f\"Patient {result['patient_id']}: {result['predicted_class_name']} ({result['confidence']:.1%})\")\n        print(f\"Stroke Risk: {result['stroke_risk_score']:.1%}\")\n    else:\n        print(f\"Failed for {result['patient_id']}: {result['error']}\")\n\"\"\")\n\nprint(\"\"\"\n# 3. CLINICAL VALIDATION WORKFLOW\n# For each patient prediction:\n\n# Step 1: Get prediction with clinical explanation\nresult = model.predict_arrhythmia_with_explanation(ecg_signal, ppg_signal)\n\n# Step 2: Extract clinical features for validation\nclinical_features = result['clinical_explanation']['clinical_features']\nheart_rate = clinical_features['hr_mean']\nrhythm_regularity = clinical_features['rhythm_regularity']\nhrv_rmssd = clinical_features['rr_rmssd']\n\n# Step 3: Get stroke risk assessment\nstroke_analysis = result['clinical_explanation']['stroke_risk_analysis']\nannual_stroke_risk = stroke_analysis['estimated_stroke_risk']  # Percentage\nrisk_category = stroke_analysis['risk_category']  # 'low', 'moderate', 'high'\nclinical_recommendations = stroke_analysis['clinical_recommendations']\n\n# Step 4: Generate report for doctor review\nclinical_report = model.generate_clinical_report(ecg_signal, ppg_signal, patient_id)\n\n# Step 5: Validation metrics\nvalidation_metrics = result['clinical_explanation']['validation_metrics']\noverall_validity = validation_metrics['overall_validity']  # 0.0-1.0\nclinical_plausibility = validation_metrics['clinical_plausibility']  # 0.0-1.0\n\"\"\")\n\nprint(\"\"\"\n# 4. SAVE/LOAD MODEL FOR PRODUCTION\n# Save trained model\nmodel.save_model(Path(\"production_model.pth\"))\n\n# Load in production environment\nfrom src.model_bridge import AdaptiveMultiModalNetwork\nproduction_model = AdaptiveMultiModalNetwork.load_model(\n    Path(\"production_model.pth\"), \n    ecg_seq_len=3600, \n    ppg_seq_len=3600\n)\n\"\"\")\n\nprint(\"\\n🎉 INTEGRATION COMPLETE!\")\nprint(\"Your self-supervised model is now ready for:\")\nprint(\"  ✅ Arrhythmia classification (5 classes)\")\nprint(\"  ✅ Stroke risk prediction (0-15% annual risk)\")\nprint(\"  ✅ Clinical explanations for doctor validation\")\nprint(\"  ✅ Automated report generation\")\nprint(\"  ✅ Full compatibility with existing pipeline\")\n\n# Save integration example\nintegration_code = \"\"\"\n# PRODUCTION INTEGRATION EXAMPLE\nfrom pathlib import Path\nfrom src.model_bridge import create_model_for_existing_pipeline\n\ndef load_trained_model():\n    '''Load the trained self-supervised model for production use'''\n    model_path = Path(\"pure_discovery_results/self_supervised_model.pth\")\n    return create_model_for_existing_pipeline(\n        ecg_seq_len=3600,\n        ppg_seq_len=3600, \n        pretrained_encoder_path=model_path\n    )\n\ndef predict_with_clinical_validation(model, ecg_signal, ppg_signal, patient_id):\n    '''Get prediction with full clinical validation'''\n    \n    # Get prediction with explanations\n    result = model.predict_arrhythmia_with_explanation(ecg_signal, ppg_signal)\n    \n    # Extract key metrics\n    prediction = {\n        'patient_id': patient_id,\n        'arrhythmia_class': result['predicted_class_name'],\n        'confidence': result['confidence'],\n        'stroke_risk_percent': result['stroke_risk_score'] * 100,\n        'clinical_report': model.generate_clinical_report(ecg_signal, ppg_signal, patient_id)\n    }\n    \n    return prediction\n\n# Usage\nmodel = load_trained_model()\nprediction = predict_with_clinical_validation(model, your_ecg, your_ppg, \"PATIENT_001\")\nprint(f\"Prediction: {prediction['arrhythmia_class']} (confidence: {prediction['confidence']:.1%})\")\nprint(f\"Stroke risk: {prediction['stroke_risk_percent']:.1f}%\")\n\"\"\"\n\nwith open(results_dir / \"integration_example.py\", \"w\") as f:\n    f.write(integration_code)\n\nprint(f\"\\n📁 Integration example saved to: {results_dir}/integration_example.py\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 🎯 How to Use in Your Existing Pipeline\n\n**Integration Instructions**: Here's how to integrate your trained self-supervised model into the existing ECG+PPG pipeline:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate comprehensive clinical report for doctor validation\nprint(\"🏥 Generating clinical report for doctor validation...\")\n\nclinical_report = adaptive_model.generate_clinical_report(\n    test_ecg, test_ppg, test_patient_id\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"📋 CLINICAL REPORT FOR DOCTOR VALIDATION\")\nprint(\"=\"*80)\nprint(clinical_report)\nprint(\"=\"*80)\n\n# Save the clinical report\nreport_path = results_dir / f\"clinical_report_{test_patient_id}.md\"\nwith open(report_path, 'w') as f:\n    f.write(clinical_report)\n\nprint(f\"\\n💾 Clinical report saved to: {report_path}\")\n\n# Demonstrate clinical feature extraction\nprint(f\"\\n🔬 Clinical Feature Analysis:\")\nclinical_explanation = result['clinical_explanation']\n\nif 'clinical_features' in clinical_explanation:\n    features = clinical_explanation['clinical_features']\n    \n    print(f\"\\n📊 Key Clinical Metrics:\")\n    print(f\"   Heart Rate: {features.get('hr_mean', 0):.1f} BPM\")\n    print(f\"   RR Interval: {features.get('rr_mean', 0):.1f} ms\")\n    print(f\"   Heart Rate Variability (RMSSD): {features.get('rr_rmssd', 0):.1f} ms\")\n    print(f\"   Rhythm Regularity: {features.get('rhythm_regularity', 0):.1f}\")\n    print(f\"   Beat Consistency: {features.get('beat_consistency', 0):.1%}\")\n\nif 'stroke_risk_analysis' in clinical_explanation:\n    stroke_analysis = clinical_explanation['stroke_risk_analysis']\n    \n    print(f\"\\n🧠 Stroke Risk Assessment:\")\n    print(f\"   Estimated Annual Risk: {stroke_analysis['estimated_stroke_risk']:.1f}%\")\n    print(f\"   Risk Category: {stroke_analysis['risk_category'].title()}\")\n    print(f\"   Confidence Level: {stroke_analysis['confidence_level']:.1%}\")\n    \n    if stroke_analysis['contributing_factors']:\n        print(f\"   Contributing Factors:\")\n        for factor in stroke_analysis['contributing_factors']:\n            print(f\"     • {factor}\")\n    \n    if stroke_analysis['clinical_recommendations']:\n        print(f\"   Recommendations:\")\n        for rec in stroke_analysis['clinical_recommendations']:\n            print(f\"     • {rec}\")\n\nprint(f\"\\n✅ Clinical analysis complete and validated for medical review!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create adaptive model for pipeline integration\nfrom src.model_bridge import AdaptiveMultiModalNetwork, create_model_for_existing_pipeline\nfrom src.clinical_explainer import ModelExplainer, ClinicalRiskFeatures, StrokeRiskPredictor\n\nprint(\"🔗 Creating adaptive model for existing pipeline integration...\")\n\n# Create model that can use our trained self-supervised encoder\npretrained_model_path = results_dir / \"self_supervised_model.pth\"\n\n# Initialize adaptive model with pre-trained encoder\nadaptive_model = AdaptiveMultiModalNetwork(\n    ecg_seq_len=ECG_FS * 10,\n    ppg_seq_len=ECG_FS * 10,\n    use_pretrained_encoder=True,\n    pretrained_encoder_path=pretrained_model_path,\n    n_arrhythmia_classes=5,  # N, S, V, F, U\n    stroke_output_dim=1\n)\n\nprint(\"✅ Adaptive model created with pre-trained encoder!\")\nprint(f\"🧠 Model ready for arrhythmia classification and stroke risk prediction\")\n\n# Move to device\nadaptive_model = adaptive_model.to(DEVICE)\n\n# Test with sample data\nprint(\"\\n🧪 Testing pipeline integration...\")\n\n# Use a sample from our dataset\ntest_idx = 0 if len(filtered_ecg) > 0 else 0\ntest_ecg = filtered_ecg[test_idx] if len(filtered_ecg) > 0 else np.random.randn(ECG_FS * 10)\ntest_ppg = filtered_ppg[test_idx] if len(filtered_ppg) > 0 else np.random.randn(PPG_FS * 10)\ntest_patient_id = filtered_ids[test_idx] if len(filtered_ids) > 0 else \"TEST_PATIENT\"\n\n# Ensure signals are the right length (10 seconds)\nif len(test_ecg) > ECG_FS * 10:\n    test_ecg = test_ecg[:ECG_FS * 10]\nif len(test_ppg) > PPG_FS * 10:\n    test_ppg = test_ppg[:PPG_FS * 10]\n\nprint(f\"📊 Testing with patient: {test_patient_id}\")\nprint(f\"   ECG signal length: {len(test_ecg)} samples ({len(test_ecg)/ECG_FS:.1f} seconds)\")\nprint(f\"   PPG signal length: {len(test_ppg)} samples ({len(test_ppg)/PPG_FS:.1f} seconds)\")\n\n# Get predictions with clinical explanations\nresult = adaptive_model.predict_arrhythmia_with_explanation(test_ecg, test_ppg)\n\nprint(f\"\\n🎯 Prediction Results:\")\nprint(f\"   Predicted Class: {result['predicted_class_name']}\")\nprint(f\"   Confidence: {result['confidence']:.1%}\")\nprint(f\"   Stroke Risk Score: {result['stroke_risk_score']:.1%}\")\n\nprint(f\"\\n📋 Class Probabilities:\")\nfor i, prob in enumerate(result['class_probabilities']):\n    class_name = ['N', 'S', 'V', 'F', 'U'][i] if i < 5 else 'Unknown'\n    print(f\"   {class_name}: {prob:.1%}\")\n\nprint(f\"\\n✅ Pipeline integration test successful!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 🔗 Pipeline Integration & Clinical Validation\n\n**IMPORTANT**: This section demonstrates how to integrate your trained self-supervised model with the existing pipeline and generate clinically explainable results for doctor validation.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}